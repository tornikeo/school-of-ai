{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Create anaconda environment\n",
    "<br>\n",
    "```bash\n",
    "conda create -n ml python=3.7.4 jupyter\n",
    "```\n",
    "Install fastai library\n",
    "<br>\n",
    "```bash\n",
    "conda install -c pytorch -c fastai fastai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supported Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Large Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From logistic regression we know that \n",
    "$$f(x, W) = \\frac{1}{1 + e^{-W^TX}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def sigm(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, sigm(x),'b', label='linspace(-10,10,100)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "if $y = 1$ then our goal is $f(x, W) \\approx 1$ and by the nature the logistic function $W^Tx >> 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the other heand if we have $y = 0$ then $f(x, W) \\approx 0$ and again by the nature of logistic function $W^Tx << 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cost function for logistic regression for the single training example:\n",
    "$$-(y\\log f(x, W) + (1 - y)\\log(1 - f(x, W)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or if we put $f(x, W)$:\n",
    "$$-y\\log(\\frac{1}{1 + e^{-W^TX}}) - (1 - y)\\log(1 - \\frac{1}{1 + e^{-W^TX}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now if $y = 1$ then only matters:\n",
    "$$-y\\log(\\frac{1}{1 + e^{-W^TX}})$$ part of equasion ($(1 - y) = 0$ and second part will be zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Denote $Z = W^Tx$ then we have \n",
    "$$\\frac{1}{1 + e^{-Z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Z = np.linspace(-6, 6, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def left_sig(x):\n",
    "    return -np.log((1 / (1 + np.exp(-x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(Z, left_sig(Z),'b', label='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When $Z$ is large then value is near zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def right_sig(x):\n",
    "    return -np.log(1 - (1 / (1 + np.exp(-x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, right_sig(Z),'b', label='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cost_1 = left_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, cost_1(Z),'b', label='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    " def cost_1(x):\n",
    "    return [max(0, -x_i + 1) for x_i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, cost_1(Z),'b', label='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cost_0 = right_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, cost_0(Z),'b', label='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cost_0(x):\n",
    "    return [max(0, x_i - 1) for x_i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x, cost_0(Z),'b', label='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Cost for logistic regression:\n",
    "$$\\min_P\\frac{1}{m} [\\sum_{i=1}^{m}y^{(i)}(-\\log f(W^{(i)}, x^{(i)}) - (1 - y^{(i)})(-\\log(1 - f(W^{(i)}, x^{(i)}))))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or with regularization:\n",
    "Cost for logistic regression:\n",
    "$$\\min_W\\frac{1}{m} [\\sum_{i=1}^{m}y^{(i)}(-\\log f(W^{(i)}, x^{(i)}) - (1 - y^{(i)})(-\\log(1 - f(W^{(i)}, x^{(i)}))))] + \\frac{\\lambda}{2m}\\sum_{i=1}^{n}w_i^2$$\n",
    "Different numbers $m$ and $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's remove $\\frac{1}{m}$ and lets $C = \\frac{1}{\\lambda}$ then:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\min_WC[\\sum_{i=1}^{m}y^{(i)}cost_1(W^{(i)^T}x^{(i)}) - (1 - y^{(i)})cost_0(W^{(i)T}x^{(i)}))] + \\frac{1}{2}\\sum_{i=1}^{n}p_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Supported Vector Machine does not output the probability:\n",
    "$$f(W, x) = \\begin{cases}1 & \\text{if }W^Tx \\ge 0 \\\\ 0 & \\text{otherwise }\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\min_W[\\sum_{i=1}^{m}y^{(i)}cost_1(W^{(i)^T}x^{(i)}) - (1 - y^{(i)})cost_0(W^{(i)^T}x^{(i)}))] + \\frac{1}{2}\\sum_{i=1}^{n}w_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "if $y = 1$ we want $W^Tx \\ge 1$ instead of $W^Tx \\ge 0$\n",
    "<br>\n",
    "if $y = 0$ we want $W^Tx \\le -1$ instead of $W^Tx \\lt 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Large Margin Classifier\n",
    "<img src=\"images/svm/svm1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximum among the minimums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us choose higher value of $C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Choise of $C$ parameter:\n",
    "<img src=\"images/svm/svm2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By the definition $C = \\frac{1}{\\lambda}$ and therefore large $C$ means small $\\lambda$ and small $C$ means the larger $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometry Behind the SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us observer optimization objectives of SVM:\n",
    "$$\\frac{1}{2}\\sum_{i=1}^{n}w_i^2$$\n",
    "$$W^Tx \\ge 1$$\n",
    "$$W^Tx \\le -1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the definition of Euclidean norm $||v|| = \\sqrt{\\sum_{i=1}^{n}v_i^2}$\n",
    "$$\\sum_{i=1}^{m}w_i^2 = ||W||^2$$\n",
    "Where $W = (w_1, w_2, \\dots, w_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Inner product of vectors:\n",
    "$$W^Tx = x_w||W||$$ \n",
    "Where $x_w$ is a length of the projection of $x$ vector on the $W$ vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we have a optimization objectives:\n",
    "$$\\frac{1}{2}||W||^2$$\n",
    "$$x_w||W|| \\ge 1$$\n",
    "$$x_w||W|| \\le -1$$\n",
    "<br>\n",
    "<img src=\"images/svm/svm3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we have a data:\n",
    "<img src=\"images/svm/svm4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could have a polinomial function:\n",
    "$$f(x, W) = w_0 + x_1w_1 + x_2w_1w_2 + \\dots x_nw_n^r$$\n",
    "<br>\n",
    "and then find:\n",
    "$$f(x, W) \\ge 0$$\n",
    "$$f(x, W) \\le -0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us put some lendmarks near the data points $l^{(1)}, l^{(2)}, \\dots l^{(m)}$ and define features $f_1, f_2, \\dots f_m$:\n",
    "<br>\n",
    "$$f_i = \\exp(-\\frac{||x - l^{(i)}||^2}{2\\sigma^2})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/svm/svm5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets choose the features by the training examples:\n",
    "$$x = (x^{(1)}, x^{(2)}, \\dots, x^{(m)})$$\n",
    "<br>\n",
    "Choose the features:\n",
    "$$f = (f^{(1)}, f^{(2)}, \\dots, f^{(m)})$$\n",
    "<br>\n",
    "by the Gaussian distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This means that we will need the $W \\in \\mathbb{R}^{m+1}$ instead of $W \\in \\mathbb{R}^{n+1}$ (with bias)\n",
    "<br>\n",
    "and our cost will be:\n",
    "$$\\min_WC[\\sum_{i=1}^{m}y^{(i)}cost_1(f^{(i)^T}x) - (1 - y^{(i)})cost_0(f^{(i)^T}x))] + \\frac{1}{2}\\sum_{i=1}^{m}w_i^2$$\n",
    "<br>\n",
    "Here parameters $m$ and $n$ are the same for training examples and for regularization respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For regularization instead of $\\sum_{i=1}^{m}w_i^2 = W^TW$ offen is used \n",
    "$$W^TMW$$ \n",
    "for some matrix $M$ for large $m$ ($m = 10000$ or even $m \\ge 1000000$) for optimization purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can aolso apply the kernels to the logistic regression but computation will be very slow, this computation reduction does not goes well with logistic regression and that's why SVM is preffered in many cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we kn ow about optimization objectives and the cost function of SVM, for practical use better use softwares which has numerical optimized implementations and we don't have to worry about them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing the C Parameter (Bias Variance Tradeoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall that:\n",
    "$$C = \\frac{1}{\\lambda}$$\n",
    "and therefore:\n",
    "- Large value of $C$ means small $\\lambda$ which implies lower bias / high variance (overfitting)\n",
    "- Small value of $C$ means large $\\lambda$ which implies higher bias / low variance (underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we use Gaussian kernel then:\n",
    "- If $\\sigma^2$ is large, features $f_i$ vary smoothly and this might cause higher bias / low variance (underfitting)\n",
    "- If $\\sigma^2$ is small, features $f_i$ vary less smoothly and this might cause lower bias / high variance (overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall that:\n",
    "$$f_i = \\exp(-\\frac{||x - l^{(i)}||^2}{2\\sigma^2})$$\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/svm/svm6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Gaussian kernel is an instance of more general Radial Basis Function:\n",
    "$$\\varphi(\\mathbf{x}) = \\varphi(\\left\\|\\mathbf{x}-\\mathbf{c}\\right\\|)$$\n",
    "or even\n",
    "$$\\varphi(\\mathbf{x}) = \\varphi(d(\\mathbf{x}-\\mathbf{c}))$$\n",
    "Which is defined as function of some metrics (Euclidean distance generated from norm or even some arbitrary metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation of SVM with scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "! pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krn_clf = svm.SVC(C=10.0, kernel='rbf', degree=3, gamma='scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??krn_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krn_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
